#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/Qwen2/modular_Qwen2.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_Qwen2.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
from typing import Callable, Optional, Union

import torch
from torch import nn

from transformers.activations import ACT2FN
from transformers.cache_utils import Cache, DynamicCache
from transformers.generation import GenerationMixin
from transformers.integrations import use_kernel_forward_from_hub
from transformers.masking_utils import create_causal_mask, create_sliding_window_causal_mask
from transformers.modeling_flash_attention_utils import FlashAttentionKwargs
from transformers.modeling_layers import GradientCheckpointingLayer
from transformers.modeling_outputs import (
    BaseModelOutputWithPast,
    CausalLMOutputWithPast,
    QuestionAnsweringModelOutput,
    SequenceClassifierOutputWithPast,
    TokenClassifierOutput,
)
from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from transformers.processing_utils import Unpack
from transformers.utils import LossKwargs, auto_docstring, can_return_tuple, logging

from transformers.models.qwen2.modeling_qwen2 import (
    Qwen2Attention, Qwen2MLP, Qwen2RMSNorm, 
    Qwen2PreTrainedModel, Qwen2RotaryEmbedding, Qwen2Model, KwargsForCausalLM,
    Qwen2ForCausalLM
)
from transformers.models.qwen2.configuration_qwen2 import Qwen2Config as OldQwen2Config
import copy
from transformers import AutoTokenizer
from typing import List, Optional
# from qwen2config import Qwen2Config
from torch._dynamo import disable

class Qwen2Config(OldQwen2Config):
    model_type = "qwen2"
    keys_to_ignore_at_inference = ["past_key_values"]

    # Default tensor parallel plan for base model `Qwen2`
    base_model_tp_plan = {
        "layers.*.self_attn.q_proj": "colwise",
        "layers.*.self_attn.k_proj": "colwise",
        "layers.*.self_attn.v_proj": "colwise",
        "layers.*.self_attn.o_proj": "rowwise",
        "layers.*.mlp.gate_proj": "colwise",
        "layers.*.mlp.up_proj": "colwise",
        "layers.*.mlp.down_proj": "rowwise",
        "reasoning_projector.gate_proj": "colwise",
        "reasoning_projector.up_proj": "colwise",
        "reasoning_projector.down_proj": "rowwise",
    }

logger = logging.get_logger(__name__)

@torch.compile()  # Explicitly compile this function
def check_if_matches_special_sequence(
    input_ids: torch.Tensor,
    special_start_sequences: torch.Tensor,
    special_end_sequences: torch.Tensor,
    tokenizer: AutoTokenizer,
) -> Optional[torch.Tensor]:
    """
    Vectorized version of special sequence matching.
    
    Looks for pattern: <start_seq> [gap_tokens] <end_seq>
    where gap can be 0-3 tokens.
    
    Args:
        input_ids: Input token sequence
        special_start_sequences: Pre-created tensor of start sequences  
        special_end_sequences: Pre-created tensor of end sequences
        tokenizer: Tokenizer (unused but kept for compatibility)
    """
    if input_ids.dim() != 1:
        input_ids = input_ids.squeeze()
        
    len_end = special_end_sequences.shape[1]
    len_start = special_start_sequences.shape[1] 
    input_len = input_ids.shape[0]
    
    # Minimum total length to contain start + end + gap
    min_total_length = len_start + len_end
    if input_len < min_total_length:
        return None

    # Move tensors to input device for comparison
    start_seqs = special_start_sequences.to(input_ids.device)
    end_seqs = special_end_sequences.to(input_ids.device)
    
    # Check if starts with any special start sequence (vectorized)
    input_start = input_ids[:len_start].unsqueeze(0)  # [1, len_start]
    start_match = (input_start == start_seqs).all(dim=1).any()  # Check full sequence match
    if not start_match:
        return None
    
    # Vectorized search through all possible gaps (0 to 3) and end sequences
    for gap in range(0, 4):
        start_idx = len_start + gap
        end_idx = len_start + len_end + gap
        
        if end_idx > input_len:
            continue
            
        # Extract potential end sequence
        candidate_end = input_ids[start_idx:end_idx]
        
        # Check if it matches any end sequence (vectorized)
        end_match = (candidate_end.unsqueeze(0) == end_seqs).all(dim=1).any()
        
        if end_match:
            # Return gap tokens (between start and end sequences)
            gap_tokens = input_ids[len_start:len_start + gap]
            return gap_tokens
    
    return None


@auto_docstring
class Qwen2PreTrainedModel(Qwen2PreTrainedModel):
    config_class = Qwen2Config
  

@auto_docstring
class Qwen2Model(Qwen2Model):
    def __init__(self, config: Qwen2Config):
        super().__init__(config)
        
        # self.reasoning_projector = Qwen2MLP(config)
        self.NUM_REASONING_LAYERS = 3
        self.reasoning_projector = nn.Sequential(
            *[Qwen2MLP(config) for layer_idx in range(self.NUM_REASONING_LAYERS)]
        )

        self.TOKENIZER = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

    @disable
    def safe_decode(self, token_ids):
        return self.TOKENIZER.decode(token_ids)

    def init_reasoning_projector_from_pretrained(self):
        last_mlp_layer = self.layers[-1].mlp
        for i in range(self.NUM_REASONING_LAYERS): 
            self.reasoning_projector[i].load_state_dict(last_mlp_layer.state_dict())

    @can_return_tuple
    @auto_docstring
    def reasoning_forward(
        self,
        all_embeddings: Optional[torch.FloatTensor] = None,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        complexity: Optional[int] = None,
        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[list[torch.Tensor], Optional[Cache]]:
        """
        Generate reasoning embeddings iteratively, where each step builds on previous context.
        
        This method performs N iterations (where N = complexity) of:
        1. Forward pass with current context + past_key_values (cache mode) OR complete sequence (non-cache mode)
        2. Extract last hidden state and project through reasoning_projector  
        3. Add reasoning embedding to context for next iteration
        4. Update past_key_values (cache mode) OR accumulate in all_embeddings (non-cache mode)
        
        all_embeddings is a list of all embeddings, and should be the basis for non-caching forward passes


        Args:
            all_embeddings: All embeddings accumulated so far (for non-cache mode)
            inputs_embeds: Initial context embeddings to start reasoning from
            past_key_values: Accumulated key/value cache from previous processing (for cache mode)
            complexity: Number of reasoning steps to generate
            use_cache: Whether to use caching or pass complete sequences
            
        Returns:
            tuple: (reasoning_embeddings, accumulated_past_key_values)
        """
        # USE_GRAD = True
        USE_GRAD = False
        # Operate in embedding space
        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)
        # Inputs embeds is the embedding of the section up to the special token
        # all_embeddings is the embedding of the entire sequence up to this point
        # when we are using caching, we want to ignore the all_embeddings,
        # but we need it for non-caching forward passes
        # Note: all_embeddings already contains inputs_embeds

        # For non-cache mode, convert all_embeddings list to tensor
        if not use_cache and all_embeddings is not None:
            all_embeddings_tensor = torch.cat(all_embeddings, dim=1)
        else:
            all_embeddings_tensor = None

        num_reasoning_steps = 5 if complexity is None else complexity 
        current_past_key_values = past_key_values
        reasoning_embeddings = []
        
        # Iterative reasoning generation - each step builds on previous context
        for step in range(num_reasoning_steps):
            # if current_past_key_values is not None:
            #     print(f"Step {step}: past_key_values shape: {current_past_key_values.key_cache[0].shape}")
            # else:
            #     print(f"Step {step}: No past_key_values")
            if USE_GRAD:
                # Forward pass to get context representation
                if use_cache:
                    # Cache mode: Use past_key_values and only pass current inputs_embeds
                    outputs = super().forward(
                        input_ids=None,
                        past_key_values=current_past_key_values,
                        inputs_embeds=inputs_embeds,
                        use_cache=True,
                        output_hidden_states=True,
                        **flash_attn_kwargs,
                    )
                else:
                    # Non-cache mode: Pass complete sequence of embeddings each time
                    # print(f"Step {step}: combined_embeddings shape: {all_embeddings_tensor.shape}")
                    outputs = super().forward(
                        inputs_embeds=all_embeddings_tensor,
                        use_cache=False,
                        output_hidden_states=True,
                        **flash_attn_kwargs,
                    )
            else:
                with torch.no_grad():
                    # Forward pass to get context representation
                    if use_cache:
                        # Cache mode: Use past_key_values and only pass current inputs_embeds
                        outputs = super().forward(
                            input_ids=None,
                            past_key_values=current_past_key_values,
                            inputs_embeds=inputs_embeds,
                            use_cache=True,
                            output_hidden_states=True,
                            **flash_attn_kwargs,
                        )
                    else:
                        # Non-cache mode: Pass complete sequence of embeddings each time
                        # print(f"Step {step}: combined_embeddings shape: {all_embeddings_tensor.shape}")
                        outputs = super().forward(
                            inputs_embeds=all_embeddings_tensor,
                            use_cache=False,
                            output_hidden_states=True,
                            **flash_attn_kwargs,
                        )
        
            # Extract context hidden state (last token processed, last layer)
            # EXPECTED: This captures the full context up to this point for reasoning
            context_hidden = outputs.hidden_states[-1][:, -1, :]  # [batch_size, hidden_size]
            
            # Generate reasoning embedding through projector (NEEDS GRADIENTS for training!)
            reasoning_embedding = self.reasoning_projector(context_hidden)  # [batch_size, hidden_size]
            
            # Store reasoning embedding
            reasoning_embeddings.append(reasoning_embedding)
            
            # Update context for next iteration
            inputs_embeds = reasoning_embedding.unsqueeze(1)  # [batch_size, 1, hidden_size]
            
            if use_cache:
                # Cache mode: Update past_key_values with accumulated context
                current_past_key_values = outputs.past_key_values
            else:
                # Non-cache mode: update all_embeddings_tensor with the reasoning embedding
                all_embeddings_tensor = torch.cat([all_embeddings_tensor, reasoning_embedding.unsqueeze(1)], dim=1)
        
        # CRITICAL: Extra forward pass to consume the last reasoning embedding, only necessary for caching
        # Non-caching modes don't need this because they pass the entire sequence each time
        # This ensures the last reasoning embedding is properly processed
        if len(reasoning_embeddings) > 0 and use_cache:
            if USE_GRAD:
                # Cache mode: Use past_key_values
                final_outputs = super().forward(
                    input_ids=None,
                    past_key_values=current_past_key_values,
                    inputs_embeds=reasoning_embeddings[-1].unsqueeze(1),  # Last reasoning embedding
                    use_cache=True,
                    **flash_attn_kwargs,
                )
                # Update past_key_values to include the last reasoning embedding
                current_past_key_values = final_outputs.past_key_values
            else:
                with torch.no_grad():
                    # Cache mode: Use past_key_values
                    final_outputs = super().forward(
                        input_ids=None,
                        past_key_values=current_past_key_values,
                        inputs_embeds=reasoning_embeddings[-1].unsqueeze(1),  # Last reasoning embedding
                        use_cache=True,
                        **flash_attn_kwargs,
                    )
                    # Update past_key_values to include the last reasoning embedding
                    current_past_key_values = final_outputs.past_key_values
            # else:
            #     # Non-cache mode: Pass complete sequence including last reasoning embedding
            #     final_outputs = super().forward(
            #         inputs_embeds=all_embeddings_tensor,
            #         use_cache=False,
            #         **flash_attn_kwargs,
            #     )
            #     # No past_key_values to update in non-cache mode
        
        return reasoning_embeddings, current_past_key_values

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],
    ) -> BaseModelOutputWithPast:
        # note about caching:
        # gradient checkpointing doesn't allow caching so we need to 
        # keep track of all of the embeddings and pass them through each time
        # this is notably slower, but should only be needed for training
        # during inference use_cache=True is much faster
        # special_start_token = 151657
        # special_end_token = 151658
        # Pre-create tensors for efficient vectorized matching
        special_start_sequences = torch.tensor([
            [27, 30940, 5854, 2450, 29],  # <implicit_thought>
            [366, 30940, 5854, 2450, 29],  # " <implicit_thought>"
            [15757, 30940, 5854, 2450, 29],
            [1784, 30940, 5854, 2450, 29],
            [22476, 30940, 5854, 2450, 29]
        ], dtype=torch.long)
        special_end_sequences = torch.tensor([
            [522, 30940, 5854, 2450, 29],
            [522, 30940, 5854, 2450, 397],
            [522, 30940, 5854, 2450, 1339],
            [522, 30940, 5854, 2450, 10370],
            [522, 30940, 5854, 2450, 14276],
            [522, 30940, 5854, 2450, 1472],
            [522, 30940, 5854, 2450, 9877],
        ], dtype=torch.long)
        # print("INSIDE BIG FORWARD")
        # print(f"attention mask shape: {attention_mask.shape if attention_mask is not None else None}")
        all_embeddings = []
        is_reasoning_embedding_mask = []
        original_past_key_values = None
        if input_ids is None:
            # print("No input_ids, running normal forward")
            return super().forward(
                input_ids=input_ids,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_values=past_key_values,
                inputs_embeds=inputs_embeds,
                use_cache=use_cache,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
                cache_position=cache_position,
                **flash_attn_kwargs,
            )
        # first we check if the input_ids contains <thought> (we actually do <|quad_start|> so it's only one token)
        else:
            # original_past_key_values = copy.deepcopy(past_key_values)
            # For now, assume batch size 1 as requested
            if input_ids.shape[0] != 1:
                raise ValueError(
                    "Currently only batch size 1 is supported for reasoning"
                )

            batch_idx = 0  # Since we only support batch size 1

            # Move sequences to device for vectorized operations
            device_start_seqs = special_start_sequences.to(input_ids.device)
            seq_len = device_start_seqs.shape[1]
            input_seq = input_ids[batch_idx]
            
            # Fast check: does input contain any start sequence tokens at all?
            start_tokens = device_start_seqs[:, 0]  # First tokens of start sequences
            has_any_start_token = torch.isin(input_seq, start_tokens).any()
            
            if not has_any_start_token:
                print("No thought start id found, running normal forward")
                return super().forward(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    position_ids=position_ids,
                    past_key_values=past_key_values,
                    inputs_embeds=inputs_embeds,
                    use_cache=use_cache,
                    output_attentions=output_attentions,
                    output_hidden_states=output_hidden_states,
                    cache_position=cache_position,
                    **flash_attn_kwargs,
                )

            # Vectorized contiguous sequence matching
            thought_start_indices = []
            input_len = input_seq.shape[0]
            
            # Slide window approach - check all possible positions for start sequences
            for i in range(input_len - seq_len + 1):
                window = input_seq[i:i + seq_len].unsqueeze(0)  # [1, seq_len]
                # Check if this window matches any start sequence
                matches = (window == device_start_seqs).all(dim=1).any()
                if matches:
                    thought_start_indices.append(i)
            # Vectorized complexity validation
            correct_complexity_indices = []  
            num_complexity_tokens = []
            complexities = []
            
            # Move special sequences to input device for vectorized operations
            device_start_seqs = special_start_sequences.to(input_ids.device)
            device_end_seqs = special_end_sequences.to(input_ids.device)
            
            for thought_start_idx in thought_start_indices:
                # Extract window for sequence matching (vectorized slice)
                window_end = min(thought_start_idx + 15, input_ids.shape[1])
                token_window = input_ids[batch_idx, thought_start_idx:window_end]
                
                # Use vectorized sequence matching
                complexity_tokens = check_if_matches_special_sequence(
                    token_window,
                    device_start_seqs,
                    device_end_seqs,
                    self.TOKENIZER,
                )
                
                if complexity_tokens is None:
                    continue
                    
                # Decode and validate complexity (minimal .decode() usage)
                complexity_str = self.safe_decode(complexity_tokens)
                if (len(complexity_str) <= 3 and len(complexity_str) > 0 and 
                    complexity_str.isdigit()):
                    complexity = int(complexity_str)
                    # FAKE COMPLEXITY VALUE
                    # complexity = 1
                    correct_complexity_indices.append(thought_start_idx)
                    num_complexity_tokens.append(len(complexity_tokens))
                    complexities.append(complexity)

            thought_start_indices = correct_complexity_indices

            if len(thought_start_indices) == 0:
                print("No full thought found, running normal forward")
                return super().forward(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    position_ids=position_ids,
                    past_key_values=past_key_values,
                    inputs_embeds=inputs_embeds,
                    use_cache=use_cache,
                    output_attentions=output_attentions,
                    output_hidden_states=output_hidden_states,
                    cache_position=cache_position,
                    **flash_attn_kwargs,
                )

            # print(f"length of thought_start_indices: {len(thought_start_indices)}")

            # print(f"thought_start_indices: {thought_start_indices}")
            # Process each section that ends with a thought_start_id
            current_past_key_values = past_key_values
            processed_tokens = 0

            for thought_start_idx, length_complexity_tokens, complexity in zip(
                thought_start_indices, num_complexity_tokens, complexities
            ):
                # print(f"thought_start_idx: {thought_start_idx}")
                # Get the tokens from the last processed position up to and including the thought_start_id
                section_start = processed_tokens
                end_of_thought_idx = (
                    thought_start_idx
                    + special_start_sequences.shape[1]  # Length of start sequence
                    + length_complexity_tokens
                    + special_end_sequences.shape[1]    # Length of end sequence
                    # + 1
                )
                end_of_thought_idx = min(end_of_thought_idx, input_ids.shape[1])

                # Extract the section input_ids
                # section_input_ids = input_ids[batch_idx:batch_idx+1, section_start:section_end]
                section_input_ids = input_ids[
                    batch_idx : batch_idx + 1, section_start:end_of_thought_idx
                ]
                # decoded_section = self.safe_decode(section_input_ids[0])
                # print(f"handling (complexity: {complexity}) section: {repr(decoded_section)}")
                # x = 1/0

                section_input_embeds = self.embed_tokens(section_input_ids)
                all_embeddings.extend(
                    [inp.unsqueeze(0) for inp in section_input_embeds]
                )
                # We mask out everything up until the reasoning embeddings
                num_to_not_mask = section_input_embeds.shape[1] - 1
                num_to_not_mask = max(num_to_not_mask, 0)
                is_reasoning_embedding_mask.extend(
                    # [False] * section_input_embeds.shape[1]
                    [False] * num_to_not_mask
                )
                is_reasoning_embedding_mask.append(True)
                # Run reasoning_forward on this section to generate reasoning embeddings
                # For non-cache mode, pass all_embeddings so reasoning_forward can build complete sequences
                # Note: section_input_embeds are already included in all_embeddings at this point
                reasoning_embeddings, updated_past_key_values = self.reasoning_forward(
                    all_embeddings=all_embeddings if not use_cache else None,
                    inputs_embeds=section_input_embeds,
                    past_key_values=current_past_key_values,
                    use_cache=use_cache,
                    complexity=complexity,
                    **flash_attn_kwargs,
                )
                
                # Add reasoning embeddings to sequence (these need gradients for training!)
                all_embeddings.extend(
                    [inp.unsqueeze(0) for inp in reasoning_embeddings]
                )
                num_reasoning_embeddings = len(reasoning_embeddings)
                num_reasoning_to_mask = max(num_reasoning_embeddings - 1, 0)
                is_reasoning_embedding_mask.extend([True] * num_reasoning_to_mask)
                is_reasoning_embedding_mask.append(False)

                # Update past_key_values with accumulated reasoning context
                current_past_key_values = updated_past_key_values
                # processed_tokens = section_end
                processed_tokens = end_of_thought_idx

            # Now process any remaining tokens after the last thought_start_id
            if processed_tokens < input_ids.shape[1]:
                remaining_input_ids = input_ids[
                    batch_idx : batch_idx + 1, processed_tokens:
                ]
                # print(f"remaining_input_ids shape: {remaining_input_ids.shape}")

                remaining_input_embeds = self.embed_tokens(remaining_input_ids)
                all_embeddings.extend(
                    [inp.unsqueeze(0) for inp in remaining_input_embeds]
                )
                is_reasoning_embedding_mask.extend(
                    [False] * remaining_input_embeds.shape[1]
                )

        # print('embedding shapes')
        # for e in all_embeddings:
        #     print(f"shape: {e.shape}")

        all_embeddings = torch.cat(
            [e.to(all_embeddings[0].device) for e in all_embeddings], dim=1
        )
        is_reasoning_embedding_mask = torch.tensor(
            is_reasoning_embedding_mask, dtype=torch.bool
        )

        # Single final forward pass with all embeddings (normal + reasoning)
        # Handle cache vs non-cache modes appropriately for gradient correctness
        
        # Update attention mask to cover all embeddings (original + reasoning)  
        if attention_mask is not None:
            difference = all_embeddings.shape[1] - attention_mask.shape[1]
            if difference > 0:
                # Add attention for reasoning embeddings
                additional_mask = torch.ones(
                    attention_mask.shape[0], difference, device=attention_mask.device
                )
                attention_mask = torch.cat([attention_mask, additional_mask], dim=1)
        
        # EXPECTED STATE: all_embeddings contains complete sequence (original + reasoning embeddings)
        # - Cache mode: We could use accumulated past_key_values, but for consistency use full sequence
        # - Non-cache mode: Must use full sequence for proper gradient flow
        
        # Always use full sequence mode for final pass to ensure proper gradients
        # This is critical for training the reasoning projector
        outputs = super().forward(
            inputs_embeds=all_embeddings,
            attention_mask=None,  # Let transformer handle attention automatically
            position_ids=None,   # Let transformer compute positions automatically
            past_key_values=None,  # Clean slate - ensures all embeddings get proper attention
            use_cache=False,     # Force non-cache to ensure gradients flow through reasoning embeddings
            cache_position=None, # Let transformer handle cache positions automatically
            **flash_attn_kwargs,
        )
        # print(f"original input_ids shape: {input_ids.shape}")
        # print(f"all_embeddings shape: {all_embeddings.shape}")
        print(f"is_reasoning_embedding_mask sum: {is_reasoning_embedding_mask.sum()}")
        # mask out the reasoning embeddings
        # print(f"outputs.last_hidden_state shape: {outputs.last_hidden_state.shape}")
        outputs.last_hidden_state = outputs.last_hidden_state[
            :, ~is_reasoning_embedding_mask, :
        ]
        # print(f"post filter outputs.last_hidden_state shape: {outputs.last_hidden_state.shape}")
        # print(outputs.last_hidden_state)
        # print(f"outputs.last_hidden_state shape: {outputs.last_hidden_state.shape}")
        # print(outouts.last_hidden_state)
        if outputs.hidden_states is not None:
            # outputs.hidden_states = tuple(
            #     hidden_state
            #     for i, hidden_state in enumerate(outputs.hidden_states)
            #     if not is_reasoning_embedding_mask[i]
            # )
            outputs.hidden_states = tuple(hs[:, ~is_reasoning_embedding_mask, :] for hs in outputs.hidden_states)
        if outputs.attentions is not None:
            # outputs.attentions = tuple(
            #     attention
            #     for i, attention in enumerate(outputs.attentions)
            #     if not is_reasoning_embedding_mask[i]
            # )
            outputs.attentions = tuple(attn[:, :, ~is_reasoning_embedding_mask, ~is_reasoning_embedding_mask] for attn in outputs.attentions)
        # print(f"RETURN PAST KEY VALUES: {outputs.past_key_values}")
        # x = 1/0
        return outputs

@auto_docstring
class Qwen2ForCausalLM(Qwen2ForCausalLM):
    
    def __init__(self, config):
        super().__init__(config)
        # super().super().__init__(config)
        self.model = Qwen2Model(config) # slightly inefficient but nicer code-wise
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

__all__ = [
    "Qwen2PreTrainedModel",
    "Qwen2Model",
    "Qwen2ForCausalLM",
]
